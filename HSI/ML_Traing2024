import numpy as np
import pandas as pd
import glob
from spectral import *
import spectral.io.envi as envi
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score, make_scorer
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.cross_decomposition import PLSRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor 
from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error,mean_absolute_error, make_scorer
from sklearn.preprocessing import StandardScaler as xscaler
from sklearn.preprocessing import LabelEncoder

#%% Step 1: Read data 
# The main code seems to use the above functions to process hyperspectral images, 
# extract features, and apply PLS regression for different numbers of components. Results are then saved to a CSV file.

disk='G:'
output_path = disk + r"\2_HSI_Root_Rot\Data_Drone\Output2024"
file_import =disk+ r"\2_HSI_Root_Rot\Data_Drone\Output2024\dataframe_final.csv"
# Read the CSVs
dataframe = pd.read_csv(file_import)

X = dataframe.iloc[1:, 2:-2].to_numpy()
Y = dataframe.iloc[1:, -1].to_numpy()
Z = dataframe.iloc[1:, 0].to_numpy()

print(X.shape)
print(Y.shape)
print(Z.shape) #Plopt number

#%% Step 2: Machine learning model
n_train_r2_mean = []
n_test_r2_mean = []
n_train_r2_std = []    
n_test_r2_std = []
n_list = []

for n in range (1, 30, 1):
    #PLS model specs
    plsmodel = PLSRegression(n_components = n, max_iter=1000)
    
    trainscores_list = []
    testscores_list = []
    
    #K-Fold cross validation
    for i in range(10):
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)
        trainscores = cross_val_score(plsmodel, X_train, Y_train, cv=5, scoring='r2')
        pls = plsmodel.fit(X_train, Y_train)
        Y_pred = pls.predict(X_test)
        testscores = r2_score(Y_test, np.reshape(Y_pred, (len(Y_pred))))
        trainscores_list.append(trainscores)
        testscores_list.append(testscores)
        
    print("Number of components : " + str(n))
    print("Training Stats")
    print("R2 mean: "+str(np.mean(trainscores_list)*100))
    print("R2 std: "+str(np.std(trainscores_list)*100))
    print("Test Stats")
    print("R2 mean: "+str(np.mean(testscores_list)*100))
    print("R2 std: "+str(np.std(testscores_list)*100))
    n_train_r2_mean.append(np.mean(trainscores_list)*100)
    n_test_r2_mean.append(np.mean(testscores_list)*100)
    n_train_r2_std.append(np.std(trainscores_list)*100)    
    n_test_r2_std.append(np.std(testscores_list)*100)
    n_list.append(n)
    
    
df = pd.DataFrame(list(zip(n_list, n_train_r2_mean, n_train_r2_std, n_test_r2_mean, n_test_r2_std)), columns =['Number of Components','Train Mean', 'Train STD', 'Test Mean', 'Train STD'])
df.to_csv(output_path+r'\Statistic.csv', index = False)

#%% Grid search for hyperparameter tuning

# 1️⃣ Split dataset once for outer evaluation
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=42)

# 2️⃣ Define model and grid of hyperparameters
pls = PLSRegression(max_iter=1000)
param_grid = {'n_components': list(range(1, 30))}

# 3️⃣ Define scorer (use R²)
r2_scorer = make_scorer(r2_score)

# 4️⃣ Grid search with 5-fold cross-validation
grid = GridSearchCV(
    estimator=pls,
    param_grid=param_grid,
    scoring=r2_scorer,
    cv=5,
    n_jobs=-1,
    return_train_score=True
)
grid.fit(X_train, Y_train)

# 5️⃣ Best model and results
print("\n✅ Best number of components:", grid.best_params_['n_components'])
print("Best cross-validated R²:", grid.best_score_)

# 6️⃣ Evaluate on independent test set
best_pls = grid.best_estimator_
Y_pred = best_pls.predict(X_test).ravel()
r2_test = r2_score(Y_test, Y_pred)
print("Independent Test R²:", r2_test)

# 7️⃣ Extract CV results for plotting
cv_results = grid.cv_results_
n_components = [p['n_components'] for p in cv_results['params']]
train_scores = cv_results['mean_train_score']
test_scores = cv_results['mean_test_score']

# 8️⃣ Plot R² vs n_components
plt.figure(figsize=(7,5))
plt.plot(n_components, train_scores, 'o-', label='Train R² (CV mean)')
plt.plot(n_components, test_scores, 's-', label='Validation R² (CV mean)')
plt.xlabel('Number of PLS Components')
plt.ylabel('R² Score')
plt.title('Grid Search for PLSRegression Components')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# 9️⃣ (Optional) Scatter plot for best model
plt.figure(figsize=(6.5,6.5))
plt.scatter(Y_test, Y_pred, s=18, alpha=0.7)
plt.plot([0,7],[0,7],'--',color='gray')
plt.xlim(0,7)
plt.ylim(0,7)
plt.xlabel('Measured Y')
plt.ylabel('Estimated Y')
plt.title(f"Best Model (n={grid.best_params_['n_components']}) | R²={r2_test:.3f}")
plt.axis('equal')
plt.show()


#%% Random Forest
X_combined=X
y_combined=Y

##################################################
# Option -2:  Split the training and test dateset
# Set random seed for reproducibility
np.random.seed(10) #50
# Split data into training and testing sets
splitRatio = 0.8
splitIdx = np.random.permutation(len(X_combined ))
trainIdx = splitIdx[:int(splitRatio * len(X_combined ))]
testIdx = splitIdx[int(splitRatio * len(X_combined )):] 
X_train, X_test = X_combined [trainIdx], X_combined [testIdx]
y_train, y_test = y_combined[trainIdx], y_combined[testIdx]
###############################################

# %% Step 3: Regression models 
# Feature Scaling for x, rather than y
sc = xscaler() # replace the standardscaler as sc
x_train = sc.fit_transform(X_train) # maybe better to change to different varibale name, standardscaler.fit_transform is scale the training dataset
x_test = sc.transform(X_test) # standardscaler.transform is to scale the test dataset. It is reasonable that both the training and test datasets need to scaled in the same method


# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': range(5,100,20),      # Number of trees
    'max_depth': [10, 20, 30],     # Maximum depth of each tree
    'max_leaf_nodes': [10, 20, 30], # Maximum number of leaf nodes
    'max_features': ['auto', 'sqrt', 'log2', None]
}

# Initialize the Random Forest Regressor model
rf_model = RandomForestRegressor(random_state=42)

# Set up the GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, 
                           cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)

# Fit the GridSearchCV
grid_search.fit(x_train, y_train)

# Get the best parameters and model from grid search
best_params = grid_search.best_params_
best_rf_model = grid_search.best_estimator_

# Predict using the best model
y_pred = best_rf_model.predict(x_test)


#%% Step 4: Evaluate the performance of the algorithms

# Evaluate model performance
r_squared = r2_score(y_test, y_pred.flatten())
rmse = root_mean_squared_error(y_test, y_pred.flatten())
cor = np.corrcoef(y_test, y_pred.flatten())

print(f'R2 on Test Data: {r_squared:.4f}')
print(f'RMSE: {rmse:.4f}')


# Plot actual vs predicted
plt.figure()
plt.scatter(y_test, y_pred, c='k', marker='o')
plt.text(6, 1.5, rf'$R^2 = {r_squared:.2f}$')
plt.text(6, 1, f'RMSE = {rmse:.2f}')
plt.xlabel('Visual Rating')
plt.ylabel('Estimated Root Rot')
# plt.title('Pea Root Rot')
plt.xlim([0, 8])
plt.ylim([0, 8])
plt.show()

# Save the model to a file
# with open(r'L:\HSI_Root_Rot\Method\rf_model2.pkl', 'wb') as f:
#     pickle.dump(rf_model, f)


#%% Step 5: Variable Importance/this could be very interesting for your paper

####### Option 1: unsorted importance
importances_rf = best_rf_model.feature_importances_  # the summation of the importance equal to 1
# Normalize VIP scores between 0 and 1
importances_rf_norm = (importances_rf - np.min(importances_rf)) / (np.max(importances_rf) - np.min(importances_rf))


filepath_1=disk+r'\2_HSI_Root_Rot\Data_Drone\HSI_20240704_PeaRootRot (FRR)\PeasRootRotHSI2024_Pika_L_1-radiance-CorrectFromFlatReference.bip.hdr'
filepath_2=disk+r'\2_HSI_Root_Rot\Data_Drone\HSI_20240704_PeaRootRot (FRR)\PeasRootRotHSI2024_Pika_L_1-radiance-CorrectFromFlatReference.bip'

# 1) Open ENVI image (memory-mapped; no full load)
img = envi.open(filepath_1, filepath_2)
# 2) Get wavelengths if present
wavelengths = np.array([float(w) for w in img.metadata.get('wavelength', [])])  

plt.figure()
plt.scatter(wavelengths, importances_rf_norm, c='k', marker='x')
mx = 1
plt.axvline(x=400, color='b')
plt.axvline(x=500, color='g')
plt.axvline(x=600, color='r')
plt.axvline(x=680, color='k')
plt.axvline(x=750, color='m')
plt.axvline(x=970, color='y')
plt.xlabel('Wavelength (nm)')
plt.ylabel('Importance of wavelength')
plt.ylim([0, mx])
plt.xlim([300, 1100])
plt.show()





#%% Classification RF

##################################################
# Option -2:  Split the training and test dateset
# Set random seed for reproducibility
np.random.seed(50)
# Split data into training and testing sets
splitRatio = 0.8
splitIdx = np.random.permutation(len(X_combined ))
trainIdx = splitIdx[:int(splitRatio * len(X_combined ))]
testIdx = splitIdx[int(splitRatio * len(X_combined )):] 
X_train, X_test = X_combined [trainIdx], X_combined [testIdx]
y_train, y_test = y_combined[trainIdx], y_combined[testIdx]
###############################################

# %% Step 3: Classification models 
# Feature Scaling for x, rather than y
sc = xscaler() # replace the standardscaler as sc
x_train = sc.fit_transform(X_train) # maybe better to change to different varibale name, standardscaler.fit_transform is scale the training dataset
x_test = sc.transform(X_test) # standardscaler.transform is to scale the test dataset. It is reasonable that both the training and test datasets need to scaled in the same method


# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': range(5,100,20),      # Number of trees
    'max_depth': [10, 20, 30],     # Maximum depth of each tree
    'max_leaf_nodes': [10, 20, 30], # Maximum number of leaf nodes
    'max_features': ['auto', 'sqrt', 'log2', None]
}

# Initialize the Random Forest Regressor model
rf_model = RandomForestClassifier (random_state=42)

# Set up the GridSearchCV
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, 
                           cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)

# Fit the GridSearchCV
grid_search.fit(x_train, y_train)

# Get the best parameters and model from grid search
best_params = grid_search.best_params_
best_rf_model = grid_search.best_estimator_

# Predict using the best model
y_pred = best_rf_model.predict(x_test)

#%% Step 4: Evaluate the performance of the algorithms

# Evaluate model performance
r_squared = r2_score(y_test, y_pred.flatten())
rmse = root_mean_squared_error(y_test, y_pred.flatten())
cor = np.corrcoef(y_test, y_pred.flatten())

print(f'R2 on Test Data: {r_squared:.4f}')
print(f'RMSE: {rmse:.4f}')


# Plot actual vs predicted
plt.figure()
plt.scatter(y_test, y_pred, c='k', marker='o')
plt.text(6, 1.5, rf'$R^2 = {r_squared:.2f}$')
plt.text(6, 1, f'RMSE = {rmse:.2f}')
plt.xlabel('Visual Rating')
plt.ylabel('Estimated Root Rot')
# plt.title('Pea Root Rot')
plt.xlim([0, 8])
plt.ylim([0, 8])
plt.show()

# Evaluate model performance using classification metrics
accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
kappa = cohen_kappa_score(y_test, y_pred)

print(f'Accuracy on Test Data: {accuracy:.4f}')
print(f'Cohen Kappa Coefficient: {kappa:.4f}')
print('Confusion Matrix:\n', cm)

# Plot Confusion Matrix
plt.figure()
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test)))
plt.xticks(tick_marks, np.unique(y_test))
plt.yticks(tick_marks, np.unique(y_test))
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()




#%% Matlab figure

from scipy.io import savemat

x = np.linspace(0, 2*np.pi, 500)
y = np.sin(x)
from matplotlib import pyplot as plt
plt.plot(x,y); plt.savefig(r"G:\2_HSI_Root_Rot\Fig_Drone\plot.png", dpi=300)

savemat(r"G:\2_HSI_Root_Rot\Fig_Drone\plot_data.mat", {"x": x, "y": y})








#%% PLS+LDA classification

# Convert labels to categorical (if needed for classification)
label_encoder = LabelEncoder()
y_combined = label_encoder.fit_transform(y_combined)  # Encoding categorical labels


##################################################
# Option -2:  Split the training and test dateset
# Set random seed for reproducibility
np.random.seed(30)
# Split data into training and testing sets
splitRatio = 0.8
splitIdx = np.random.permutation(len(X_combined ))
trainIdx = splitIdx[:int(splitRatio * len(X_combined ))]
testIdx = splitIdx[int(splitRatio * len(X_combined )):] 
X_train, X_test = X_combined [trainIdx], X_combined [testIdx]
y_train, y_test = y_combined[trainIdx], y_combined[testIdx]
###############################################

# %% Step 3: Regression models 
# Test Number of latent variables
accuracy = []
for Ncom in range(1, 41):
    pls = PLSRegression(n_components=Ncom)
    pls.fit(X_train, y_train)
    
    # Apply LDA on the reduced components (PLS scores)
    X_train_pls = pls.transform(X_train)  # Reduced components
    X_test_pls = pls.transform(X_test)
    
    lda = LinearDiscriminantAnalysis()
    lda.fit(X_train_pls, y_train)
    
    # Predict on the test set
    y_pred = lda.predict(X_test_pls)
    accuracy.append(accuracy_score(y_test, y_pred))
    print(f'Accuracy on Test Data for {Ncom} components: {accuracy[-1]}')

# Plot accuracy vs. number of components
plt.figure()
plt.plot(range(1, 41), accuracy, 'ok')
plt.xlabel('Number of components in PLS-LDA')
plt.ylabel('Accuracy')
plt.title('Selection of Components')
plt.show()

# Select optimal number of components
num_com = np.argmax(accuracy) + 1

# Train final model with optimal number of components
pls = PLSRegression(n_components=num_com)
pls.fit(X_train, y_train)

X_train_pls = pls.transform(X_train)
X_test_pls = pls.transform(X_test)

lda = LinearDiscriminantAnalysis()
lda.fit(X_train_pls, y_train)

y_pred = lda.predict(X_test_pls)


# Evaluate model
r2 = r2_score(y_test, y_pred)
rmse_s = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'R^2 on Test Data: {r2}')

# Plot actual vs. predicted values
plt.figure()
plt.scatter(y_test, y_pred, c='k', marker='o')
plt.text(6, 1.5, rf"$R^2 = {r2:.2f}$")
plt.text(6, 1, f'RMSE={rmse_s:.2f}')
plt.xlabel('Visual Rating')
plt.ylabel('Estimated Root Rot')
# plt.title('Pea Root Rot')
plt.xlim([0, 8])
plt.ylim([0, 8])
plt.show()

# Evaluate the model
accuracy_final = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
print(f'Final Accuracy on Test Data: {accuracy_final}')
print('Confusion Matrix:')
print(conf_matrix)

# Plot Confusion Matrix
plt.figure()
plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')
plt.title('Confusion Matrix')
plt.xlabel('Estimated Root Rot')
plt.ylabel('Visual Rating')
plt.colorbar()
plt.xticks(np.arange(len(label_encoder.classes_)), ['Low','Moderate', 'High'])
plt.yticks(np.arange(len(label_encoder.classes_)), ['Low','Moderate', 'High'], rotation=90)
plt.show()



# Normalize the confusion matrix to display percentages
conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentage

plt.figure()
plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')
plt.title('Confusion Matrix')
plt.xlabel('Estimated Root Rot')
plt.ylabel('Visual Rating')
plt.colorbar()
classes = ['Low', 'Moderate', 'High']
plt.xticks(np.arange(len(classes)), classes)
plt.yticks(np.arange(len(classes)), classes, rotation=90)

# Add percentage text annotations
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        plt.text(j, i, f'{conf_matrix_normalized[i, j]:.1f}%',
                 ha='center', va='center', color='black' if conf_matrix_normalized[i, j] < 50 else 'white')

plt.show()


#%% Calculate Variable Importance in Projection (VIP)
W0 = pls.x_weights_ / np.sqrt(np.sum(pls.x_weights_ ** 2, axis=0))
p = X.shape[1]
sumSq = np.sum(pls.x_scores_ ** 2, axis=0) * np.sum(pls.y_loadings_ ** 2, axis=0)
vipScore = np.sqrt(p * np.sum(sumSq * (W0 ** 2), axis=1) / np.sum(sumSq))

plt.figure()
plt.scatter(waveleth[:-3], vipScore, c='k', marker='x')
mx = 4.5
plt.axvline(x=400, color='b')
plt.axvline(x=500, color='g')
plt.axvline(x=600, color='r')
plt.axvline(x=680, color='k')
plt.axvline(x=750, color='m')
plt.axvline(x=970, color='y')
plt.xlabel('Wavelength (nm)')
plt.ylabel('Importance of wavelength')
plt.ylim([0, mx])
plt.xlim([300, 1100])
plt.show()