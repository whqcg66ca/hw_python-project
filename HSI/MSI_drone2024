import numpy as np
import pandas as pd
import glob
from spectral import *
import spectral.io.envi as envi
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import r2_score, mean_squared_error,  mean_absolute_error
from osgeo import gdal
#import cv2
#import pygal
#import seaborn as sns
#from pygal.style import Style
#from imblearn.under_sampling import RandomUnderSampler
# Use band195 and band135 to mask out the soil

disk='G:'

#%% Step 1: Read MSI imagery 
filepath_1 = disk + r'\2_HSI_Root_Rot\Data_Drone\AltumPT_20240704_PeaRootRot (FRR)\AltumPT_20240704_Pea.tif'

# --- Open dataset ---
ds = gdal.Open(filepath_1)
if ds is None:
    raise FileNotFoundError(f'Cannot open: {filepath_1}')

rows, cols, bands = ds.RasterYSize, ds.RasterXSize, ds.RasterCount
print(f'Size: {rows} x {cols} x {bands}')

# Helper to read + apply scale/offset
def read_band_as_float(ds, idx):
    b = ds.GetRasterBand(idx)
    arr = b.ReadAsArray().astype(np.float32)

    # Apply scale/offset if present (common in radiance/reflectance products)
    scale = b.GetScale()
    offset = b.GetOffset()
    if scale is None:  scale = 1.0
    if offset is None: offset = 0.0
    arr = arr * scale + offset

    # Replace inf with NaN to simplify masking
    arr[~np.isfinite(arr)] = np.nan
    # Optionally treat very small negatives as 0
    arr[arr < 0] = np.nan
    return arr, b.GetNoDataValue()

# Known Altum PT RGB order (R=4, G=3, B=2)
R, R_nodata = read_band_as_float(ds, 4)
G, G_nodata = read_band_as_float(ds, 3)
B, B_nodata = read_band_as_float(ds, 2)

def robust_stretch(x, p_low=2, p_high=98, ignore_zeros=True):
    x = x.copy()
    # Build a mask of valid pixels
    valid = np.isfinite(x)
    if ignore_zeros:
        valid &= (x != 0)

    # If almost everything is invalid, bail out gracefully
    if valid.sum() < 100:
        # fallback: just clip to [0, 1]
        return np.clip(x, 0, 1)

    lo = np.nanpercentile(x[valid], p_low)
    hi = np.nanpercentile(x[valid], p_high)

    # Guard against degenerate ranges
    if not np.isfinite(lo) or not np.isfinite(hi) or hi - lo < 1e-6:
        # Try using min/max of valid pixels
        lo = np.nanmin(x[valid])
        hi = np.nanmax(x[valid])
        if not np.isfinite(lo) or not np.isfinite(hi) or hi - lo < 1e-6:
            # Last resort: assume reflectance range [0,1]
            lo, hi = 0.0, 1.0

    x = (x - lo) / (hi - lo)
    x = np.clip(x, 0, 1)
    return x

R_s = robust_stretch(R)
G_s = robust_stretch(G)
B_s = robust_stretch(B)

RGB = np.dstack([R_s, G_s, B_s]).astype(np.float32)

print("Band stats after scaling (min/median/max):")
for name, a in zip(['R','G','B'], [R, G, B]):
    vals = a[np.isfinite(a)]
    if vals.size:
        print(name, float(np.nanmin(vals)), float(np.nanmedian(vals)), float(np.nanmax(vals)))
    else:
        print(name, "no finite values")

plt.figure(figsize=(8,8))
plt.imshow(RGB)
plt.title('RGB composite (R=4, G=3, B=2) — robust stretch')
plt.axis('off')
plt.show()

#%% Step 2: Apply OSAVI mask and show RGB composite of masked data

# --- Load all bands (once) ---
img = np.zeros((rows, cols, bands), dtype=np.float32)
for i in range(bands):
    img[:, :, i] = read_band_as_float(ds, i + 1)[0]

# --- Compute OSAVI ---
RED_BAND, NIR_BAND = 4, 6     # adjust if needed
OSAVI = (1.16 * (img[:, :, NIR_BAND-1] - img[:, :, RED_BAND-1]) /
         (img[:, :, NIR_BAND-1] + img[:, :, RED_BAND-1] + 0.16 + 1e-9))

# --- Mask based on OSAVI range ---
mask2d = (OSAVI > 0.32297996) & (OSAVI < 1.16007769)
print("Mask fraction (kept):", np.mean(mask2d))

# --- Apply mask ---
img_msk = img.copy()
img_msk[~mask2d, :] = np.nan

# --- Build RGB (bands 4,3,2) ---
R = img_msk[:, :, 4-1]
G = img_msk[:, :, 3-1]
B = img_msk[:, :, 2-1]
RGB = np.dstack([robust_stretch(R), robust_stretch(G), robust_stretch(B)])

# Optional: gray out masked regions for context
valid = np.isfinite(RGB).all(axis=2)
RGB[~valid] = 0.2

# --- Display ---
plt.figure(figsize=(8, 8))
plt.imshow(RGB)
plt.title("OSAVI-Masked RGB Composite (R=4, G=3, B=2)")
plt.axis('off')
plt.show()

img=img_msk
#%% Step 3: Extract the polt average for path 1

# This function seems to create a dataframe from a given text file and image shape information, merging it with another dataframe.
classes = np.genfromtxt(disk+r'\2_HSI_Root_Rot\Data_Drone\Label_studio\Altum_2024\classes.txt', delimiter=' ')
lbpath=disk+r'\2_HSI_Root_Rot\Data_Drone\Label_studio\Altum_2024\labels\184a46e0-AltumPT_20240704_Pea.txt'
output_path=disk+r'\2_HSI_Root_Rot\Data_Drone\Output2024\MSI'

d = pd.read_csv(lbpath, sep=' ',names=['class','x','y','width','height','x0','x1','y0','y1','id', 'class_name']).fillna('NaN')

img_shape=img.shape

d['class_name'] = classes[d['class']].astype(int)
d['x0'] = np.round(img_shape[1]*(d['x']-(d['width']/2))).astype(int)
d['x1'] = np.round(img_shape[1]*(d['x']+(d['width']/2))).astype(int)
d['y0'] = np.round(img_shape[0]*(d['y']-(d['height']/2))).astype(int)
d['y1'] = np.round(img_shape[0]*(d['y']+(d['height']/2))).astype(int)

string = lbpath.split("\\")[-1]
id = string.split('.')[0]
id = id.split('_')[-1]
id = id.split('_')[-1]
for index in range(d.shape[0]):
    d['id'] = id

d_path1=d


#def extract_spectra(filepath_1, filepath_2, txtfile_path):
#alllabel = []
#label_list = []
allhsi = []
allnames = []
hsi_list = []
spectral_list = []
name_list = []

empty_array = np.zeros((500, 500, 7))
#img = envi.open(filepath_1, filepath_2)
#img_shape = (img.shape[0], img.shape[1])
#print(img_shape)
data = d_path1

for i in range(data.shape[0]):
    #label = data['response'][i]
    print(i)
    spectral_file =  img[data['y0'][i]:data['y1'][i]:, data['x0'][i]:data['x1'][i]:]
  
    for x in range(spectral_file.shape[0]):
        for y in range(spectral_file.shape[1]):
            allhsi.append(spectral_file[x,y,:])
            #alllabel.append(label)
            allnames.append(data['id'][i])
    
    spectrum = np.nanmean(spectral_file, axis=0)
    spectrum = np.nanmean(spectrum, axis=0)
    empty_array[int((empty_array.shape[0]-spectral_file.shape[0])/2):int((empty_array.shape[0]+spectral_file.shape[0])/2),int((empty_array.shape[1]-spectral_file.shape[1])/2):int((empty_array.shape[1]+spectral_file.shape[1])/2),:] = spectral_file
    
    #label_list.append(int(label))
    hsi_list.append(empty_array)
    spectral_list.append(spectrum)
    name_list.append(data['id'][i])
    #return(label_list, hsi_list, spectral_list, name_list, allhsi, alllabel, allnames)
    
#df = pd.DataFrame(spectral_list,columns=wavelengths)
#df.insert(0, 'class_name', data['class_name'])   # add as first column
#df.to_csv(output_path+r'\spectral_avg11.csv', index=False)
  
wavelengths=[634,475,560,668,717,842,11000]
  # --- Build DataFrame and put class_name first ---
df = pd.DataFrame(spectral_list, columns=wavelengths)
# Align class_name rows with spectra we kept
df.insert(0, 'class_name', d.loc[:len(df)-1, 'class_name'].to_list())
df.insert(1, 'id', name_list)

# Save
df.to_csv(output_path + r'\MSIspectral_avg1.csv', index=False)

#%% Step5: Combine path1 and path2

# Replace with your actual file paths
file1 =disk+ r"\2_HSI_Root_Rot\Data_Drone\Output2024\MSI\MSIspectral_avg1.csv"

# Read the CSVs
df1 = pd.read_csv(file1)


# Print summary information
print("=== File 1 ===")
print(df1.head(), "\n")
print("Shape:", df1.shape)


# Example: merge or concatenate if they share same structure
d_combine = df1
print("\n=== Combined ===")
print(d_combine.head())
print("Combined shape:", d_combine.shape)
# Save
d_combine.to_csv(output_path + r'\spectral_avg_com.csv', index=False)


d2 = pd.read_csv(disk+r'\2_HSI_Root_Rot\Data_Drone\Rating_Protocol\2024 disease nursery map and disease ratings_CV_used2.csv',  header=0, names=['class_name', 'response']).fillna('NaN')
d1 = pd.merge(d_combine, d2, on='class_name', how='inner')
print(d1)

d1.to_csv(output_path+'\MSIdataframe_final.csv', index = False)



# The code includes necessary libraries for numerical operations (numpy), 
# image processing (cv2), data manipulation (pandas), file handling (glob), 
# hyperspectral analysis (spectral), machine learning (sklearn), data visualization (seaborn, matplotlib, pygal), and imbalanced data handling (imblearn).



# custom_style = Style(
#   background='transparent',
#   plot_background='transparent',
#   foreground='black',
#   foreground_strong='#53A0E8',
#   foreground_subtle='#630C0D',
#   opacity='.6',
#   opacity_hover='.9',
#     font_family='googlefont:Roboto',
#     label_font_size = 24,
#     major_label_font_size = 24,
#     value_font_size = 24,
#     legend_font_size = 24,
#   transition='400ms ease-in',
#   colors=('#f54242', '#f5b342', '#75f542', '#4842f5', '#c542f5', '#f542bc', '#f54287', '#f54242', '#e9f542'))


#%% Machine learning model
# The main code seems to use the above functions to process hyperspectral images, 
# extract features, and apply PLS regression for different numbers of components. Results are then saved to a CSV file.

disk='D:'
file_import =disk+ r"\2_HSI_Root_Rot\Data_Drone\Output\dataframe_final.csv"
# Read the CSVs
dataframe = pd.read_csv(file_import )

X = dataframe.iloc[1:, 2:-1].to_numpy()
Y = dataframe.iloc[1:, -1].to_numpy()
Z = dataframe.iloc[1:, 0].to_numpy()

print(X.shape)
print(Y.shape)
print(Z.shape)


n_train_r2_mean = []
n_test_r2_mean = []
n_train_r2_std = []    
n_test_r2_std = []
n_list = []

for n in range (1, 30, 1):
    #PLS model specs
    plsmodel = PLSRegression(n_components = n, max_iter=1000)
    
    trainscores_list = []
    testscores_list = []
    
    #K-Fold cross validation
    for i in range(2):
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)
        trainscores = cross_val_score(plsmodel, X_train, Y_train, cv=5, scoring='r2')
        pls = plsmodel.fit(X_train, Y_train)
        Y_pred = pls.predict(X_test)
        testscores = r2_score(Y_test, np.reshape(Y_pred, (len(Y_pred))))
        trainscores_list.append(trainscores)
        testscores_list.append(testscores)
        
    print("Number of components : " + str(n))
    print("Training Stats")
    print("R2 mean: "+str(np.mean(trainscores_list)*100))
    print("R2 std: "+str(np.std(trainscores_list)*100))
    print("Test Stats")
    print("R2 mean: "+str(np.mean(testscores_list)*100))
    print("R2 std: "+str(np.std(testscores_list)*100))
    n_train_r2_mean.append(np.mean(trainscores_list)*100)
    n_test_r2_mean.append(np.mean(testscores_list)*100)
    n_train_r2_std.append(np.std(trainscores_list)*100)    
    n_test_r2_std.append(np.std(testscores_list)*100)
    n_list.append(n)
    
    
df = pd.DataFrame(list(zip(n_list, n_train_r2_mean, n_train_r2_std, n_test_r2_mean, n_test_r2_std)), columns =['Number of Components','Train Mean', 'Train STD', 'Test Mean', 'Train STD'])
df.to_csv(output_path+'\Statistic.csv', index = False)

#%% Scattering plot
# tracking across n
n_train_r2_mean, n_test_r2_mean, n_train_r2_std, n_test_r2_std, n_list = [], [], [], [], []

best = {
    "r2": -np.inf,
    "n": None,
    "Y_test": None,
    "Y_pred": None
}

for n in range(1, 30):
    plsmodel = PLSRegression(n_components=n, max_iter=1000)

    trainscores_list = []
    testscores_list  = []

    for i in range(2):
        # use a fixed seed for reproducibility if you like
        X_train, X_test, Y_train, Y_test = train_test_split(
            X, Y, test_size=0.2, shuffle=True, random_state=42+i
        )
        trainscores = cross_val_score(plsmodel, X_train, Y_train, cv=5, scoring='r2')
        pls = plsmodel.fit(X_train, Y_train)
        Y_pred = pls.predict(X_test).ravel()

        test_r2 = r2_score(Y_test, Y_pred)
        trainscores_list.append(trainscores)
        testscores_list.append(test_r2)

        # keep best run overall for plotting later
        if test_r2 > best["r2"]:
            best.update({
                "r2": test_r2,
                "n": n,
                "Y_test": Y_test.copy(),
                "Y_pred": Y_pred.copy()
            })

    print("Number of components :", n)
    print("Training Stats")
    print("R2 mean: ", np.mean(trainscores_list)*100)
    print("R2 std:  ", np.std(trainscores_list)*100)
    print("Test Stats")
    print("R2 mean: ", np.mean(testscores_list)*100)
    print("R2 std:  ", np.std(testscores_list)*100)

    n_train_r2_mean.append(np.mean(trainscores_list)*100)
    n_test_r2_mean.append(np.mean(testscores_list)*100)
    n_train_r2_std.append(np.std(trainscores_list)*100)
    n_test_r2_std.append(np.std(testscores_list)*100)
    n_list.append(n)

# ---- Scatter plot for the best model/run ----
y_true = best["Y_test"]
y_pred = best["Y_pred"]

# Metrics
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae  = mean_absolute_error(y_true, y_pred)
r2   = best["r2"]

plt.figure(figsize=(6.5, 6.5))
plt.scatter(y_true, y_pred, s=18, alpha=0.7, edgecolor='none')

# 1:1 line
lo = float(min(np.min(y_true), np.min(y_pred)))
hi = float(max(np.max(y_true), np.max(y_pred)))
plt.plot([lo, hi], [lo, hi], linestyle='--', linewidth=1)

plt.xlabel("Measured Y")
plt.ylabel("Estimated Y")
plt.title(f"PLS Measured vs Estimated (n={best['n']})\nR²={r2:.3f}, RMSE={rmse:.3f}, MAE={mae:.3f}")
plt.axis('equal')
# plt.xlim(lo, hi)
# plt.ylim(lo, hi)
plt.xlim(0, 7)
plt.ylim(0, 7)
plt.tight_layout()

# Optional: save figure (set your own path)
# plt.savefig(r"D:\2_HSI_Root_Rot\Data_Drone\Output\pls_measured_vs_estimated.png", dpi=200)

plt.show()