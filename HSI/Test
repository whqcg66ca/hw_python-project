import numpy as np
import pandas as pd
import glob
from spectral import *
import spectral.io.envi as envi
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.cross_decomposition import PLSRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor 
from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error,mean_absolute_error, make_scorer
from sklearn.preprocessing import StandardScaler as xscaler



from sklearn.model_selection import GroupKFold, GroupShuffleSplit, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import TransformedTargetRegressor
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import r2_score
import numpy as np

#%% Step 1: Read data 
# The main code seems to use the above functions to process hyperspectral images, 
# extract features, and apply PLS regression for different numbers of components. Results are then saved to a CSV file.

disk='G:'
output_path = disk + r"\2_HSI_Root_Rot\Data_Drone\Output2024"
file_import =disk+ r"\2_HSI_Root_Rot\Data_Drone\Output2024\dataframe_final.csv"
# Read the CSVs
dataframe = pd.read_csv(file_import)

X = dataframe.iloc[1:, 2:-1].to_numpy()
Y = dataframe.iloc[1:, -1].to_numpy()
Z = dataframe.iloc[1:, 0].to_numpy()

print(X.shape)
print(Y.shape)
print(Z.shape) #Plopt number

#%% Test
# --- Groups: use your plot IDs in Z ---
groups = np.asarray(Z)  # ensure 1D array, same length as Y

# Outer split by groups (so no plot leakage)
outer = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
(train_idx, test_idx), = outer.split(X, Y, groups=groups)
X_train, X_test = X[train_idx], X[test_idx]
Y_train, Y_test = Y[train_idx], Y[test_idx]
groups_train = groups[train_idx]

# Pipeline: scale X, PLS inside CV; scale Y via TransformedTargetRegressor
pls_pipe = Pipeline([
    ("xscaler", StandardScaler(with_mean=True, with_std=True)),
    ("pls", PLSRegression(max_iter=1000))
])

model = TransformedTargetRegressor(
    regressor=pls_pipe,
    transformer=StandardScaler(with_mean=True, with_std=True)
)

param_grid = {"regressor__pls__n_components": list(range(1, 31))}
inner_cv = GroupKFold(n_splits=5)

grid = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring="r2",
    cv=inner_cv,
    n_jobs=-1,
    return_train_score=True
)

# >>> IMPORTANT: pass groups here <<<
grid.fit(X_train, Y_train, groups=groups_train)

print("Best params:", grid.best_params_)
print("CV R²:", grid.best_score_)

best_model = grid.best_estimator_
Y_pred = best_model.predict(X_test).ravel()
print("Test R²:", r2_score(Y_test, Y_pred))
print("Y_test std:", np.std(Y_test), " | Y_pred std:", np.std(Y_pred))



plt.figure(figsize=(6.5,6.5))
plt.scatter(Y_test, Y_pred, s=18, alpha=0.7)
plt.plot([0,7],[0,7],'--',color='gray')
plt.xlim(0,7)
plt.ylim(0,7)
plt.xlabel('Measured Y')
plt.ylabel('Estimated Y')
plt.title(f"Best Model (n={grid.best_params_['n_components']}) | R²={r2_test:.3f}")
plt.axis('equal')
plt.show()